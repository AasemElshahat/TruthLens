\documentclass[12pt, a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage[german, english]{babel} % Use 'german' if you are writing in German

\usepackage[margin=2.5cm]{geometry}
\usepackage{microtype}

% --- Title Page Information ---
% (Your university will likely have a specific format for this)
\title{A Comparative Analysis of Large Language Models (GPT-4, Gemini, DeepSeek) for Factual Claim Extraction and Verification}
\author{Aasem Elshahat}
\date{\today} % Or your submission date

\usepackage[hidelinks, breaklinks=true]{hyperref} % Makes citations clickable without ugly red boxes
\usepackage{array}      % Required for custom column formatting
\usepackage{tabularx}   % Required for auto-width tables that fill the page
\usepackage{breakcites}
\usepackage{amsmath} 

% --- Begin Document ---
\begin{document}

% --- Title Page ---
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \includegraphics[width=0.6\textwidth]{bht_logo_horizontal.png}
        \vspace{1.5cm}

        % \textbf{\LARGE Berliner Hochschule für Technik (BHT)} \\
        % \vspace{0.5cm}
        % \textbf{\Large Medieninformatik/ Fachbereich VI} \\
        
        % \vspace{1.5cm}

        \textbf{\Huge A Comparative Analysis of Large Language Models (GPT-4, Gemini, DeepSeek) for Factual Claim Extraction and Verification}

        \vspace{1.5cm}

        {\Large \textbf{Bachelor's Thesis}}

        \vspace{1.5cm}

        Submitted by: \\
        \vspace{0.3cm}
        \textbf{\Large Aasem Elshahat} \\
        \vspace{0.5cm}
        Matriculation Number: 954435 \\

        \vspace{2cm}

        \textbf{Supervisors:} \\
        Prof. Dr. Siamak Haschemi \\
        Dipl.-Inf. (FH) Markus Schubert

        \vspace{2cm}

        \Large
        \today

    \end{center}
\end{titlepage}

% --- Abstract / Acknowledgements ---

\begin{abstract}
    % A brief, one-paragraph summary of the entire thesis.
    % will write this last.
\end{abstract}

\clearpage

% --- Table of Contents ---
% This command generates the Gliederung
\tableofcontents
\clearpage

% --- List of Figures and Tables ---
% (Good academic practice)
\listoffigures
\clearpage
\listoftables
\clearpage

% --- Main Thesis Content ---



\chapter{Introduction}
\label{ch:introduction}

The advent of Large Language Models (LLMs) has fundamentally transformed the landscape of Natural Language Processing (NLP) and information retrieval. Models such as OpenAI's GPT-4 \cite{GPT4omini}, Google's Gemini \cite{Gemini25}, and DeepSeek \cite{DeepSeekV3.2} have demonstrated unprecedented capabilities in generating human-like text, summarizing complex documents, and answering open-domain questions. However, the widespread adoption of these systems in critical domains—such as journalism, legal research, and education—is severely hindered by their tendency to generate plausible but factually incorrect information, a phenomenon widely known as ``hallucination'' \cite{Ji2023Survey}.

As LLMs are increasingly integrated into search engines, the ability to automatically verify the factual veracity of their outputs has become a paramount research challenge. This thesis addresses this challenge by \textbf{adapting} an open-source decoupled two-phase framework for automated fact-checking and \textbf{benchmarking} the performance of state-of-the-art LLMs (GPT-4, Gemini, DeepSeek) within this architecture.

\section{Motivation}
    
    \subsection{The Problem of LLM Hallucinations}
    While LLMs are fluent and coherent, they lack an inherent understanding of truth. They operate as probabilistic systems, predicting the next token based on statistical patterns learned during training rather than querying a structured knowledge base of verified facts. Consequently, these models often conflate correct information with outdated data, common misconceptions, or complete fabrications \cite{Ji2023Survey}.
    
    The risk is not merely academic; in high-stakes applications, hallucinations can lead to the propagation of misinformation, legal liabilities, and the erosion of user trust. For instance, an LLM might generate a summary of a news article that accurately captures the tone but invents specific dates or statistics. This ``fluency-factuality gap''—where the text reads perfectly but is factually wrong—makes manual verification difficult and time-consuming for human users.

    \subsection{The Need for Automated Fact-Checking}
    Manual fact-checking is unscalable given the volume of content generated by LLMs. Traditional fact-checking pipelines often rely on human annotators or static knowledge graphs, which cannot keep pace with the dynamic nature of information on the web \cite{FEVER}.
    
    Recent approaches have proposed using LLMs themselves as evaluators (LLM-as-a-Judge) \cite{LLMJudge}. However, existing evaluation frameworks often treat fact-checking as a monolithic task, asking a model to ``verify this text'' in a single step. This approach obscures the root cause of errors: did the model fail to notice a verifiable claim (extraction error), or did it fail to verify the claim against evidence (verification error)? 
    
    To address this, there is a need for a granular, agent-based evaluation framework that decouples \textit{extraction} from \textit{verification}. By isolating these phases, researchers can identify whether a model is better suited for the high-recall task of identifying claims or the high-precision task of judging them against ground truth.

\section{Objective and Research Questions}
    
    \subsection{Thesis Objective}
    The primary objective of this thesis is to perform a comparative analysis of three leading Large Language Models—\textbf{GPT-4}, \textbf{Google Gemini}, and \textbf{DeepSeek}—to determine their efficacy in an automated fact-checking pipeline.

    \begin{table}[ht]
        \centering
        \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{2.5cm}|>{\raggedright\arraybackslash}p{3.5cm}|>{\raggedright\arraybackslash}X|}
            \hline
            \textbf{Provider} & \textbf{Model Name} & \textbf{Version / Endpoint} \\ 
            \hline
            OpenAI & GPT-4o mini & \texttt{gpt-4o-mini-2024-07-18} \\ 
            \hline
            Google & Gemini 2.5 Flash & \texttt{gemini-2.5-flash-001} \\ 
            \hline
            DeepSeek & DeepSeek-V3.2 & \texttt{deepseek-chat} (V3.2 API) \\ 
            \hline
        \end{tabularx}
        \caption{Specifications of the Large Language Models evaluated in this study.}
        \label{tab:models_specs}
    \end{table}
    
    Unlike previous studies that evaluate end-to-end performance, this research introduces and utilizes the \textbf{TruthLens Framework}, a novel agent-based architecture implementing a \textbf{Decoupled Two-Phase Evaluation Approach}. We leverage the \textit{BingCheck} dataset \cite{BingCheck} to assess each model's performance in:
    \begin{enumerate}
        \item \textbf{Phase 1 (Extraction):} Identifying verifiable factual claims from unstructured sentences \cite{Metropolitansky2025}.
        \item \textbf{Phase 2 (Verification):} Determining the accuracy of the LLMs in assessing the veracity of those claims against a standardized ground truth, utilizing a search-augmented verification approach \cite{SAFE2024}.
    \end{enumerate}
    This thesis aims to establish which model offers the optimal balance between precision (trustworthiness) and recall (coverage) for building reliable automated fact-checking agents.
    
    \subsection{Research Questions}
    To achieve the stated objective, this thesis answers the following three research questions (RQs):
    
    \begin{description}
        \item[RQ1: Claim Extraction] Which LLM most accurately identifies verifiable factual claims from text at the sentence level? \\
        \textit{Specifically, how do the models compare in terms of Recall (avoiding missed claims) versus Precision (avoiding subjective noise)?}
        
        \item[RQ2: Claim Verification] Which LLM most accurately assesses the veracity of identified claims when provided with claims?
        
        \item[RQ3: Qualitative Analysis] What are the common failure modes for each LLM in the fact-checking pipeline? \\
        \textit{Do certain models tend to be overly conservative (silence) or overly confident (hallucination)?}
    \end{description}
    
\section{Structure of the Thesis}
    The remainder of this thesis is structured as follows:
    
    \textbf{Chapter \ref{ch:background}} provides the theoretical background on Large Language Models, the phenomenon of hallucination, and the state-of-the-art in automated fact-checking, including a review of the SAFE framework \cite{SAFE2024} and the BingCheck dataset.
    
    \textbf{Chapter \ref{ch:methodology}} details the experimental methodology. It defines the \texttt{truthlens} framework, the decoupled evaluation strategy, and the specific definitions of metrics such as F1-Score and atomic claims used throughout the study.
    
    \textbf{Chapter \ref{ch:implementation}} describes the technical implementation of the evaluation pipeline, including the Python-based agent architecture, the integration of LLM APIs, and the automated benchmarking scripts.
    
    \textbf{Chapter \ref{ch:results}} presents the quantitative results of the experiments. It offers a statistical comparison of GPT-4, Gemini, and DeepSeek across both extraction and verification phases, highlighting the trade-offs between cost, speed, and accuracy across multiple experimental runs.
    
    \textbf{Chapter \ref{ch:discussion}} interprets the findings, discussing the implications of the ``precision-recall trade-off'' observed in the models. It also provides a qualitative error analysis of specific failure cases.
    
    \textbf{Chapter \ref{ch:conclusion}} summarizes the contributions of this work and suggests future directions for agent-based fact-checking systems.




\chapter{Background and State of the Art}
\label{ch:background}

    This chapter establishes the theoretical foundations of the thesis. It reviews the phenomenon of hallucination in Large Language Models (LLMs), analyzes the state-of-the-art methodologies for factual claim extraction and verification, and introduces the theoretical basis for the \texttt{truthlens} framework.


    \section{Large Language Models and Factuality}
    The capabilities of Large Language Models (LLMs) have scaled dramatically in recent years, driven by advancements in transformer architectures and the expansion of pre-training datasets. However, these models remain prone to ``hallucination''—the generation of text that is fluent and coherent but factually incorrect \cite{Ji2023Survey}.
    
    Early research in Natural Language Generation (NLG) primarily categorized these errors relative to a source document (e.g., in summarization tasks). This produced the standard distinction between \textbf{Intrinsic} and \textbf{Extrinsic} hallucinations (Table \ref{tab:hallucination_types}).
    
    \begin{table}[ht]
        \centering
        \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{3.5cm}|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|}
            \hline
            \textbf{Type} & \textbf{Definition} & \textbf{Example Scenario} \\ 
            \hline
            \textbf{Intrinsic Hallucination} & 
            The generated output directly contradicts the source material or input context. & 
            Input: ``The cat is black.'' \newline Output: ``The cat is white.'' \\ 
            \hline
            \textbf{Extrinsic Hallucination} & 
            The generated output cannot be verified from the source (neither supported nor contradicted), effectively constituting a fabrication. & 
            Input: ``The cat is black.'' \newline Output: ``The cat is named Felix.'' (Source never mentions a name). \\ 
            \hline
        \end{tabularx}
        \caption{Classic classification of Hallucinations as defined by \cite{Ji2023Survey}.}
        \label{tab:hallucination_types}
    \end{table}
    
    However, as LLMs have evolved into open-ended agents that rely on world knowledge rather than just a single input document, recent literature argues for a refined taxonomy. \cite{Huang2024Survey} distinguish between two dominant failure modes in modern Generative AI:
    
    \begin{enumerate}
        \item \textbf{Faithfulness Hallucination:} The model fails to follow user instructions or maintains internal inconsistency (e.g., self-contradiction).
        \item \textbf{Factuality Hallucination:} The model generates content that contradicts verifiable real-world facts. This manifests as \textit{Factual Fabrication} (inventing non-existent entities) or \textit{Factual Contradiction} (misstating established relations).
    \end{enumerate}
    
    This thesis focuses specifically on detecting and mitigating \textbf{Factuality Hallucinations}. Unlike faithfulness errors, which can often be solved via prompt engineering, factuality errors require external verification pipelines—such as the \texttt{truthlens} framework proposed in this study—to audit the model's output against reliable ground truth.

    
    \section{Phase 1: Factual Claim Extraction}
    The first step in any fact-checking pipeline is identifying \textit{what} needs to be checked. Raw LLM output often interweaves verifiable facts with opinions, reasoning, and conversational filler. Evaluating the text as a monolithic block often leads to imprecise results.
    
    \subsection{Literature Review: The Atomic Claim Approach}
    State-of-the-art research suggests that effective evaluation requires breaking down long-form text into "atomic claims." An atomic claim is defined as a specific, verifiable proposition that is fully self-contained \cite{Metropolitansky2025}.
    
    Recent work by Metropolitansky and Larson (2025) introduced \textbf{Claimify}, a framework that formalized this process. They identified that naive sentence splitting is insufficient because it strips necessary context (e.g., resolving pronouns like "he" or "it"). Their research proposed a multi-stage pipeline consisting of:
    \begin{enumerate}
        \item \textbf{Selection:} Filtering out subjective or non-verifiable sentences.
        \item \textbf{Disambiguation:} Resolving coreferences (e.g., changing "He released the album" to "Kendrick Lamar released the album") to make sentences self-contained.
        \item \textbf{Decomposition:} Breaking complex compound sentences into individual atomic facts.
    \end{enumerate}
    This thesis adopts the Claimify methodology for Phase 1, positing that higher-quality extraction leads to more accurate verification downstream.
    
    \subsection{The BingCheck Dataset}
    To evaluate extraction performance, a rigorous ground truth is required. This study leverages the \textbf{BingCheck} corpus \cite{BingCheck}, a high-quality dataset originally designed for evaluating Retrieval-Augmented Generation (RAG) systems across diverse domains such as finance, history, and science.
    
    However, the original BingCheck dataset focuses on document-level verification. For the specific task of atomic claim extraction (Phase 1), this thesis utilizes the sentence-level annotations derived from BingCheck and published by \cite{Metropolitansky2025} alongside the Claimify framework. This derived corpus provides expert-labeled binary classifications for sentences (contains claim / does not contain claim), serving as the standard ground truth for our precision and recall measurements.
    
    \section{Phase 2: Factual Claim Verification}
    Once atomic claims are extracted, they must be verified against external evidence. Relying on an LLM's internal parametric knowledge is insufficient due to the "knowledge cutoff" problem \cite{Cheng2024} and the risk of reinforcing existing hallucinations.
    
    \subsection{Literature Review: Search-Augmented Verification}
    The prevailing standard for automated verification is \textbf{Search-Augmented Factuality Evaluation (SAFE)}, proposed by researchers at Google DeepMind \cite{SAFE2024}. SAFE introduces the paradigm of using an LLM agent to interact with a search engine (e.g., Google Search) to verify claims.
    
    The SAFE framework operates on a "reasoning loop":
    \begin{enumerate}
        \item \textbf{Query Generation:} The model formulates search queries based on the claim.
        \item \textbf{Evidence Retrieval:} Search results are parsed to find supporting or refuting evidence.
        \item \textbf{Reasoning:} The model compares the claim against the retrieved evidence to issue a verdict (Supported, Irrelevant, or Not Supported).
    \end{enumerate}
    Wei et al. (2024) demonstrated that LLM agents using this method can achieve human-level performance in rating long-form factuality, significantly outperforming human crowdsourced annotators in terms of cost and scalability. This thesis adapts the SAFE logic into the verification agent of the \texttt{truthlens} framework.

    \section{The \texttt{truthlens} Framework}
    To empirically evaluate the performance of GPT-4, Gemini, and DeepSeek, this thesis employs the \textbf{TruthLens} framework. TruthLens is an adaptation of the open-source \textit{ClaimeAI} repository, which implements a decoupled fact-checking pipeline.
    
    For this study, the original framework—which relied exclusively on GPT-4o-mini—was extended to support a multi-model architecture, allowing for the direct comparison of different Large Language Models as the underlying cognitive engine.

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{truthlens_architecture.png} 
        \caption{The \texttt{truthlens} Decoupled Framework Architecture. Phase 1 extracts atomic claims, which are passed to Phase 2 for search-augmented verification.}
        \label{fig:truthlens_architecture}
    \end{figure}
    
    \subsection{Agent 1: The Claim Extractor}
    The Claim Extractor agent implements the \textit{Claimify} methodology described in Section 2.2. It utilizes a graph-based workflow to process unstructured text through specific nodes for selection, disambiguation, and decomposition \cite{Metropolitansky2025}. By adhering to this rigorous extraction logic, the agent ensures that only atomic, context-independent claims are passed to the next phase.
    
    \subsection{Agent 2: The Claim Verifier}
    The Claim Verifier agent implements the search-augmented reasoning loop described in Section 2.3 \cite{SAFE2024}. Designed as an autonomous agent, it iteratively generates search queries, evaluates the sufficiency of retrieved evidence, and issues a final verdict (Supported, Refuted, or Insufficient Information) based on the specific capabilities of the LLM being tested.



\chapter{Methodology}
\label{ch:methodology}
    This chapter details the experimental framework designed to answer the research questions. It defines the specific Large Language Models (LLMs) selected for comparison, the structure of the TruthLens agentic framework, and the rigorous metric definitions used to evaluate performance in both extraction and verification phases.
    

    \section{Experimental Framework}
    To perform a fair and reproducible comparative analysis, this study utilizes \textbf{TruthLens}—an experimental adaptation of the open-source \textit{ClaimeAI} repository \cite{ClaimeAI_Repo}.\footnote{Original repository available at: \url{https://github.com/BharathxD/ClaimeAI}}
    
    While the original repository provided a baseline implementation for single-model execution (utilizing only GPT-4o-mini), this study introduces significant architectural extensions required for comparative analysis:
    \begin{itemize}
        \item \textbf{Multi-Provider Abstraction:} Refactoring the underlying LLM interface to support the execution of Google Gemini and DeepSeek models alongside OpenAI.
        \item \textbf{Decoupled Benchmarking Pipeline:} Developing scripts to isolate Phase 1 (Extraction) and Phase 2 (Verification) metrics for independent evaluation.
        \item \textbf{Dataset Adaptation:} Integrating the BingCheck ground-truth dataset to enable quantitative precision/recall measurement for the claim extraction phase.
    \end{itemize}
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=1.0\textwidth]{images/experimental_pipeline.png}
        \caption{Experimental Pipeline: Data flow from sampling through evaluation phases to final metrics analysis.}
        \label{fig:experimental_pipeline}
    \end{figure}

    \subsection{Experimental Design and Variables}
    The experiment is designed as a comparative study where the \textbf{Independent Variable} is the Large Language Model used as the cognitive engine for the agents. The \textbf{Dependent Variables} are the performance metrics (Precision, Recall, F1-Score, and Accuracy) achieved by each model.
    
    To ensure internal validity, the following controls were strictly enforced:
    \begin{itemize}
        \item \textbf{Identical Prompts:} All three models received the exact same system instructions and few-shot examples for both extraction and verification tasks. No model-specific prompt engineering was applied.
        \item \textbf{Deterministic Sampling:} To minimize variability in generation, the temperature settings were standardized (where applicable) to prioritize the deterministic outputs.
    \end{itemize}
    
    \subsection{LLM Providers}
    The study benchmarks three models representing different architectural philosophies (Proprietary vs. Open Weights) and cost profiles. The specific versions used for all experiments are listed in Table \ref{tab:models_specs} (see Chapter 1).
    
    \section{Dataset Creation and Sampling}
    \subsection{Source Dataset (BingCheck)}
    The source text for this experiment is derived from the \textbf{BingCheck} dataset \cite{BingCheck}. BingCheck was selected because it contains high-quality, human-annotated fact-checking instances spanning diverse domains (Science, History, Finance).
    
    \subsection{Sampling Strategy (Phase 1)}
    Due to the computational intensity of agentic workflows, a stratified random sample was utilized. Using a fixed random seed (\texttt{random\_state=42}) for reproducibility, $N=150$ sentences were sampled from the source corpus. 
    
    \subsection{Ground Truth Generation}
    A critical challenge in automated fact-checking is establishing a reliable "Gold Standard." This study adopts a hybrid approach:
    
    \begin{itemize}
        \item \textbf{Phase 1 (Extraction):} The ground truth for extraction was derived from the expert annotations published by the authors of the \textit{Claimify} framework \cite{Metropolitansky2025}. Each of the 150 sampled sentences possesses a binary label (\textit{True/False}) indicating whether it contains a factual claim.
        
        \item \textbf{Phase 2 (Verification):} For the verification phase, a curated set of 100 atomic claims was derived from the extraction output across all models. To ensure a fair evaluation of the model's reasoning and safety behaviors, the ground truth was established via manual human annotation using a \textbf{three-label schema}:
        \begin{itemize}
            \item \textbf{Supported:} The claim is explicitly confirmed by authoritative sources.
            \item \textbf{Refuted:} The claim is explicitly contradicted by authoritative sources.
            \item \textbf{Insufficient Information:} No reliable evidence could be found to either support or refute the claim (e.g., unverifiable private information or ambiguous predictions).
        \end{itemize}
        This alignment ensures that if a model correctly identifies a claim as unverifiable, it is penalized not as an error but rewarded as a correct assessment of uncertainty.
    \end{itemize}
    
    \section{Decoupled Two-Phase Evaluation}
    The \texttt{truthlens} framework executes the evaluation in two distinct, sequential phases.
    
    \subsection{Phase 1: Claim Extraction (The "Recall" Task)}
    The objective of Phase 1 is to convert unstructured input text into a list of \textit{atomic claims}. An atomic claim must be verifiable and decontextualized.
    For this phase, the \texttt{claim\_extractor} agent processes the 150 sampled sentences. The output is evaluated as a binary classification task: for every sentence in the ground truth, did the model correctly identify it as containing a claim?
    
    \subsection{Phase 2: Claim Verification (The "Precision" Task)}
    The objective of Phase 2 is to verify the accuracy of the claims. The \texttt{claim\_verifier} agent utilizes a search-augmented loop (Search $\rightarrow$ Read $\rightarrow$ Reason) to compare the claim against web evidence. 
    The output is a multi-class classification: \textit{Supported}, \textit{Refuted}, or \textit{Insufficient Information}. The model's verdict is compared against the manually annotated ground truth to calculate accuracy.


    \section{Evaluation Metrics}
    Reliable measurement requires distinct metrics for each phase.
    
    \subsection{Phase 1: Claim Extraction Metrics (Binary)}
    In Phase 1, the task is treated as a binary classification problem where the \textbf{Positive Class} represents the presence of a verifiable claim, and the \textbf{Negative Class} represents its absence (e.g., subjective opinions or chitchat). To fully capture the Precision-Recall trade-off (RQ1), metrics are calculated for both classes.
    
    \subsubsection{Positive Class Metrics (Target: Factual Claims)}
    These metrics evaluate the model's ability to correctly identify factual content:
    \begin{itemize}
        \item \textbf{Precision (+):} $\frac{TP}{TP + FP}$ \\
        \textit{Interpretation:} \textbf{Resistance to Noise.} Of the sentences the LLM classified as "factual," how many actually were? High precision implies the model avoids hallucinating claims in subjective text.
        
        \item \textbf{Recall (+):} $\frac{TP}{TP + FN}$ \\
        \textit{Interpretation:} \textbf{Coverage.} Of the sentences that actually contained facts, how many did the LLM find?
        
        \item \textbf{F1-Score (+):} The harmonic mean of Precision (+) and Recall (+). This is the primary ranking metric for extraction performance.
    \end{itemize}
    
    \subsubsection{Negative Class Metrics (Target: Non-Factual Content)}
    These metrics evaluate the model's ability to correctly reject non-factual content:
    \begin{itemize}
        \item \textbf{Negative Predictive Value (NPV):} $\frac{TN}{TN + FN}$ \\
        \textit{Interpretation:} When the model predicts a sentence is non-factual, how often is it correct?
        
        \item \textbf{Specificity / Recall (-):} $\frac{TN}{TN + FP}$ \\
        \textit{Interpretation:} \textbf{Filtering Capability.} Of the subjective/non-factual sentences in the dataset, how many did the model correctly ignore? This directly measures the model's ability to filter out subjective noise.
    
        \item \textbf{F1-Score (-):} $2 \times \frac{NPV \times Specificity}{NPV + Specificity}$ \\
        \textit{Interpretation:} The harmonic mean for the negative class. A high score here indicates a balanced ability to filter noise without aggressively rejecting valid facts.
    \end{itemize}
    
    \subsection{Phase 2: Claim Verification Metrics (Multi-Class)}
    In Phase 2, the model must classify a claim into one of three categories: \textit{Supported}, \textit{Refuted}, or \textit{Insufficient Information}. 
    
    The primary metric is \textbf{Accuracy}, calculated as the ratio of correct verdicts to total claims:
    \begin{equation}
        Accuracy = \frac{\text{Correct Verdicts}}{\text{Total Claims Verified}}
    \end{equation}
    
    Additionally, we calculate the \textbf{Macro-Averaged F1-Score} across all three classes. This ensures that the model is evaluated not just on its ability to verify facts, but also on its ability to correctly identify when evidence is missing (Recall for the ``Insufficient'' class), preventing bias towards the majority class.
    
    \section{Reliability and Reproducibility}
    Given that Large Language Models are non-deterministic by nature, a single experimental run may not accurately reflect the model's stable performance characteristics. To mitigate this variability and ensure statistical reliability, the entire evaluation pipeline (Phase 1 and Phase 2) is executed in \textbf{three independent runs} ($k=3$).
    
    For each metric $M$ (Precision, Recall, F1-Score), the reported result is the mean across all three runs:
    \begin{equation}
        M_{final} = \frac{1}{k} \sum_{i=1}^{k} M_i
    \end{equation}
    This multi-run approach allows for the calculation of standard deviation to quantify the stability of each model's extraction and verification capabilities.

    \section{Threats to Validity}
    This section acknowledges the limitations that may affect the generalizability and interpretation of results.

    \subsection{Internal Validity}
    \begin{itemize}
        \item \textbf{LLM Non-Determinism:} Large Language Models produce variable outputs even with identical inputs. This threat is mitigated by executing $k=3$ independent runs and reporting mean performance with standard deviation.
        
        \item \textbf{API Version Drift:} Cloud-hosted models may be updated by providers during the experimental period. All experiments were conducted within a concentrated timeframe (December 2024) to minimize this effect.
    \end{itemize}

    \subsection{External Validity}
    \begin{itemize}
        \item \textbf{Dataset Scope:} The BingCheck dataset, while spanning diverse domains (Science, History, Finance), represents a specific distribution of factual claims. Results may not generalize to highly specialized domains (e.g., advanced medical or legal claims).
        
        \item \textbf{Sample Size:} The evaluation uses $N=150$ sentences for extraction and $N=100$ claims for verification. While sufficient for comparative analysis, larger-scale studies may reveal different performance patterns.
    \end{itemize}

    \subsection{Construct Validity}
    \begin{itemize}
        \item \textbf{Ground Truth Subjectivity:} The Phase 2 ground truth relies on manual annotation, which may introduce subjective bias. This threat was mitigated through two measures: (1) using a clearly defined three-label schema with explicit criteria for each verdict category, and (2) documenting the authoritative source URL for each annotation in the dataset, enabling full traceability and independent verification of the ground truth labels.
        
        \item \textbf{Search Engine Variability:} The Brave Search API may return different results over time as web content changes, potentially affecting verification reproducibility.
    \end{itemize}
    

\chapter{Implementation}
\label{ch:implementation}

This chapter details the technical realization of the \texttt{truthlens} framework. It describes the system architecture, the engineering of the LLM abstraction layer required for multi-model support, and the development of the automated benchmarking pipeline used to execute the experiments.

    \section{System Architecture}
    
    The \texttt{truthlens} framework is built upon a modular, multi-agent architecture designed to decouple the task of claim extraction from claim verification. The system is implemented in Python using \textbf{LangGraph} \cite{LangGraph}, a library for building stateful, multi-actor applications with Large Language Models (LLMs).
    
    While the initial prototype (based on the open-source \textit{ClaimeAI} repository) provided a baseline implementation for single-model execution, this thesis required a robust, model-agnostic framework capable of comparative benchmarking. Consequently, the architecture was significantly refactored to support dynamic model switching, batch processing, and automated metric collection.
    
    The high-level architecture consists of three primary components:
    \begin{enumerate}
        \item \textbf{The Agent Core:} Two specialized autonomous agents (Extractor and Verifier) that execute the logic defined in the \textit{Claimify} and \textit{SAFE} methodologies.
        \item \textbf{The Abstraction Layer:} A custom-built interface that standardizes communication between the agents and different LLM providers (OpenAI, Google, DeepSeek).
        \item \textbf{The Benchmarking Engine:} A suite of automation scripts designed to execute large-scale experiments, handle API failures, and serialize results for analysis.
    \end{enumerate}
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.9\textwidth]{images/system_architecture.png}
        \caption{High-level architecture of the TruthLens Multi-Agent System. Adapted from \cite{ClaimeAI_Repo}.}
        \label{fig:system_arch}
    \end{figure}
    
    \subsection{Agent Workflows}
    The system utilizes two distinct \textbf{stateful cyclic graphs} to manage the state of the fact-checking process.
    
    \subsubsection{The Claim Extractor Agent}
    The extraction phase is implemented as a five-stage pipeline. As illustrated in Figure \ref{fig:extractor_flow}, the agent receives raw text and processes it through sequential nodes:
    \begin{itemize}
        \item \textbf{Sentence Splitter:} Segments text while preserving context windows (5 preceding sentences).
        \item \textbf{Selection Node:} Filters out non-factual sentences using a consensus voting mechanism ($k=3$ completions) to reduce false positives.
        \item \textbf{Disambiguation Node:} Resolves coreferences (e.g., replacing ``he'' with ``Kendrick Lamar'').
        \item \textbf{Decomposition Node:} Breaks complex compound sentences into atomic claims.
        \item \textbf{Validation Node:} Performs a final syntax check on the extracted claims.
    \end{itemize}
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\textwidth]{images/extractor_workflow.png}
        \caption{The Claim Extraction Workflow implementing the Claimify methodology. Adapted from \cite{ClaimeAI_Repo}.}
        \label{fig:extractor_flow}
    \end{figure}
    
    \subsubsection{The Claim Verifier Agent}
    While the extractor operates as a linear pipeline, the Claim Verifier implements an \textbf{iterative reasoning loop} designed to mimic human fact-checking behavior (Figure \ref{fig:verifier_flow}). Defined in \texttt{claim\_verifier/agent.py}, this agent utilizes a state graph that allows it to gather evidence dynamically until sufficient information is obtained.
    
    The workflow consists of four distinct nodes:
    \begin{itemize}
        \item \textbf{Generate Search Query:} Analyzes the atomic claim and formulates precise, neutral search queries (e.g., converting ``He won the election'' to ``winner of 2024 US presidential election'').
        \item \textbf{Retrieve Evidence:} Executes the generated queries using the configured search provider and parses the results into structured evidence objects.
        \item \textbf{Search Decision (The ``Brain''):} This is the critical control node. It evaluates the retrieved evidence against the claim to determine sufficiency. As implemented in \texttt{nodes/search\_decision.py}, the model outputs a structured decision:
        \begin{itemize}
            \item \textit{Needs More Evidence (True):} The agent generates a new, refined query and loops back to the retrieval step.
            \item \textit{Needs More Evidence (False):} The agent proceeds to the final evaluation.
        \end{itemize}
        To prevent infinite loops, a hard limit (configurable via \texttt{max\_iterations}) forces the agent to conclude after $N=3$ attempts.
        \item \textbf{Evaluate Evidence:} The final node synthesizes all accumulated evidence to issue a verdict (\textit{Supported}, \textit{Refuted}, or \textit{Insufficient Information}) along with a reasoning trace.
    \end{itemize}

    The critical logic for the \textit{Search Decision} node is enforced via a structured Pydantic model \cite{Pydantic}, which compels the LLM to justify its decision before stopping. As shown in Listing \ref{lst:search_decision}, the model must explicitly identify missing aspects before requesting more evidence.

\begin{verbatim}
class SearchDecisionOutput(BaseModel):
    """Evidence sufficiency assessment for claim verification."""

    needs_more_evidence: bool = Field(
        description="Return True if: evidence is limited, contradictory, 
        or lacks authoritative sources. Return False only when 
        evidence is comprehensive."
    )
    missing_aspects: list[str] = Field(
        default_factory=list,
        description="Specific aspects that need more evidence coverage."
    )
\end{verbatim}
\label{lst:search_decision}

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\textwidth]{images/verifier_workflow.png}
        \caption{The Claim Verifier Workflow implementing the SAFE reasoning loop. Adapted from \cite{ClaimeAI_Repo}.}
        \label{fig:verifier_flow}
    \end{figure}

    \section{LLM Abstraction Layer}
    
    A critical contribution of this thesis is the development of a unified \textbf{LLM Abstraction Layer}. The original codebase was tightly coupled to the OpenAI API, making comparative analysis impossible. To address this, the system was refactored to implement the \textbf{Strategy Pattern} \cite{Gamma1994}, allowing the underlying cognitive engine to be swapped at runtime without modifying the agent logic.
    
    \subsection{Provider Interface}
    We defined an abstract base class, \texttt{LLMProvider}, which enforces a common interface for all model interactions. This ensures that regardless of whether the model is proprietary (GPT-4) or open-weights (DeepSeek), the agents interact with it using a standardized protocol.
    
\begin{verbatim}
class LLMProvider(ABC):
    @abstractmethod
    def invoke(
        self,
        model_name: str = None,
        temperature: float = 0.0,
        completions: int = 1,
    ) -> BaseChatModel:
        """Get LLM instance with specified configuration."""
        pass
\end{verbatim}
    
    Concrete implementations were developed for three providers:
    \begin{itemize}
        \item \textbf{OpenAIProvider:} Interfaces with \texttt{gpt-4o-mini}.
        \item \textbf{GeminiProvider:} Interfaces with Google's \texttt{gemini-2.5-flash}, utilizing specific safety setting overrides to prevent false refusals on sensitive news topics.
        \item \textbf{DeepSeekProvider:} Interfaces with \texttt{deepseek-chat} (V3.2).
    \end{itemize}
    
    \subsection{DeepSeek Integration Strategy}
    A significant technical challenge was DeepSeek's lack of native support for complex structured output (JSON Schema enforcement) compared to OpenAI's function calling. To resolve this, a \textbf{custom formatting strategy} was implemented within the provider.
    
    This implementation acts as a middleware that:
    \begin{enumerate}
        \item Intercepts the prompt and injects a strict JSON schema definition into the system message.
        \item Appends a ``force-formatting'' instruction to the end of the user prompt.
        \item Post-processes the raw text output to extract and validate the JSON object before passing it back to the agent.
    \end{enumerate}
    
    This ``polyfill'' implementation was crucial for enabling the DeepSeek model to function within the strict type-checking requirements of the LangGraph framework.

    \subsection{Search Provider Extension}
    The original repository included a basic search interface supporting expensive neural search APIs like Exa (formerly Metaphor) and Tavily. To enable large-scale benchmarking within the budget constraints of this study, we extended this module to support the \textbf{Brave Search API} \cite{BraveSearchAPI}.
    
    The implementation in \texttt{search/provider.py} abstracts these distinct backends behind a unified asynchronous interface. This allows the agents to switch between providers without logic changes:
    
    \begin{enumerate}
        \item \textbf{Brave Search:} Integrated specifically for this thesis to provide cost-effective, keyword-based retrieval.
        \item \textbf{Exa \& Tavily:} Retained from the original codebase for legacy support, though not used in the primary bulk experiments due to cost factors.
    \end{enumerate}
    
    To handle the inherent instability of network requests during large-scale benchmarking (e.g., 450 total runs), the provider was wrapped with an exponential backoff retry mechanism to ensure transient API failures do not invalidate experimental data.

    \section{Benchmark Automation}
    
    To conduct the comparative analysis, a robust benchmarking pipeline was developed to automate the execution of experiments across the dataset.
    
    \subsection{Phase 1: Extraction Automation}
    The script \texttt{run\_extraction\_phase.py} manages the batch processing of the $N=150$ sentences. It iterates through the target models (OpenAI, Gemini, DeepSeek), dynamically injecting the appropriate \texttt{LLMProvider} into the extraction graph.
    
    To ensure the statistical reliability defined in the methodology ($k=3$ runs), the automation script includes a \texttt{generate\_unique\_filename} utility. This ensures that subsequent experimental runs do not overwrite previous data, automatically versioning output files (e.g., \texttt{dataset\_run2.csv}, \texttt{dataset\_run3.csv}) to preserve a complete audit trail of all iterations.
    
    \subsection{Phase 2: Verification Automation}
    The verification phase is orchestrated by \texttt{run\_verification\_phase.py}. This script loads the standardized benchmark claims generated in Phase 1 and feeds them into the verification agent.
    
    Key engineering features of the verification runner include:
    \begin{itemize}
        \item \textbf{Per-Claim State Persistence:} Given that verifying a single claim may involve multiple search iterations (taking 30-60 seconds), the script saves results to the CSV after \textit{every} single transaction. This ``checkpointing'' strategy prevents data loss in the event of API timeouts or rate limit disconnects.
        \item \textbf{Dynamic Schema Updates:} The script dynamically creates distinct columns for each model (e.g., \texttt{gpt4\_verdict}, \texttt{gemini\_verdict}) within the same master dataset, facilitating direct row-by-row comparison during analysis.
        \item \textbf{Error Resilience:} Failed verifications (e.g., due to strict content filters on sensitive topics) are caught and logged as \texttt{None}, allowing the pipeline to continue processing remaining claims without crashing.
    \end{itemize}
    
    \subsection{Data Storage and Serialization}
    The complex, nested output of the LangGraph agents (containing claim objects and metadata) is serialized and appended to the master dataset. The resulting CSV file utilizes a \textbf{wide-format structure}, where each input sentence is preserved as a row, and the extraction results for each model (GPT-4, Gemini, DeepSeek) are stored in distinct columns. This structure facilitates direct, side-by-side comparison of model performance on identical inputs.

    \section{Analysis Pipeline}
    
    The final component of the implementation is the dual-phase analysis suite, consisting of \texttt{analyze\_extraction.py} and \texttt{analyze\_verification.py}. These scripts perform the statistical evaluation of the raw data against the ground truth.
    
    \subsection{Phase 1 Analysis (Extraction)}
    The extraction analysis script evaluates the binary classification performance of the models. It implements the logic to:
    \begin{enumerate}
        \item Load the ground truth labels (Binary: Contains Claim / No Claim) from the dataset.
        \item Parse the model's JSON output to determine the predicted label (Did the model extract $\ge 1$ claim?).
        \item Calculate the confusion matrix (TP, FP, TN, FN) across all 150 sentences.
        \item Compute the final Precision, Recall, and F1-Scores for both positive and negative classes, aggregating results across the three experimental runs to ensure statistical significance.
    \end{enumerate}

    \subsection{Phase 2 Analysis (Verification)}
    The verification analysis script assesses the accuracy of the models' fact-checking verdicts. Unlike the binary extraction phase, this is evaluated as a multi-class classification problem. The script:
    \begin{enumerate}
        \item Ingests the benchmark dataset containing the 100 standardized claims and their manual ground truth annotations (\textit{Supported}, \textit{Refuted}, or \textit{Insufficient Info}).
        \item Normalizes the model's text output (e.g., mapping ``The claim is false'' to the label \texttt{Refuted}).
        \item Calculates the overall \textbf{Accuracy} (percentage of correct verdicts).
        \item Computes the \textbf{Macro-Averaged F1-Score} to ensure balanced performance across all three truth categories, preventing the metric from being skewed by the majority class.
    \end{enumerate}

\chapter{Results and Evaluation}
\label{ch:results}
    \section{Phase 1: Claim Extraction Performance}
        \subsection{Quantitative Metric Comparison (Table)}
        \subsection{Visual Comparison (Bar Charts)}
        \subsection{Reliability Analysis Across Multiple Runs}
    \section{Phase 2: Claim Verification Performance}
        \subsection{Quantitative Metric Comparison (Table)}
        \subsection{Visual Comparison (Bar Charts)}
    \section{Experimental Cost and Time Analysis}

\chapter{Discussion}
\label{ch:discussion}
    \section{Comparative Analysis of LLMs}
        \subsection{Best Performing LLM for Extraction}
        \subsection{Best Performing LLM for Verification}
    \section{Qualitative Error Analysis}
        \subsection{Common Failures in Claim Extraction}
        \subsection{Common Failures in Claim Verification (Analysis of \texttt{reason} field)}
    \section{Limitations of the Study}
        \subsection{Impact of Sample Size (N=150)}
        \subsection{Constraints of the \texttt{truthlens} Agents}

\chapter{Conclusion and Future Work}
\label{ch:conclusion}
    \section{Summary of Findings}
        \subsection{Answering Research Question 1 (Extraction)}
        \subsection{Answering Research Question 2 (Verification)}
        \subsection{Answering Research Question 3 (Qualitative)}
    \section{Future Work}
        \subsection{Scaling the Experiment}
        \subsection{Evaluating Other Agent Architectures}

% --- Appendices ---
\appendix
\chapter{Appendix}
    \section{Full Benchmark Results Table}
    \section{Benchmark Runner Script (\texttt{run\_my\_thesis.py})}
    \section{Analysis Script (\texttt{analyze\_results.py})}

% --- Bibliography ---
\clearpage

\bibliographystyle{apalike}
\bibliography{references}

\end{document}
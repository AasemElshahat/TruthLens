\documentclass[12pt, a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage[german, english]{babel} % Use 'german' if you are writing in German

\usepackage[margin=2.5cm]{geometry}
\usepackage{microtype}

% --- Title Page Information ---
% (Your university will likely have a specific format for this)
\title{A Comparative Analysis of Large Language Models (GPT-4, Gemini, DeepSeek) for Factual Claim Extraction and Verification}
\author{Aasem Elshahat}
\date{\today} % Or your submission date

\usepackage[hidelinks, breaklinks=true]{hyperref} % Makes citations clickable without ugly red boxes
\usepackage{array}      % Required for custom column formatting
\usepackage{tabularx}   % Required for auto-width tables that fill the page
\usepackage{breakcites}
\usepackage{amsmath} 

% --- Begin Document ---
\begin{document}

% --- Title Page ---
\maketitle

% --- Abstract / Acknowledgements ---

\begin{abstract}
    % A brief, one-paragraph summary of the entire thesis.
    % will write this last.
\end{abstract}

\clearpage

% --- Table of Contents ---
% This command generates the Gliederung
\tableofcontents
\clearpage

% --- List of Figures and Tables ---
% (Good academic practice)
\listoffigures
\clearpage
\listoftables
\clearpage

% --- Main Thesis Content ---



\chapter{Introduction}
\label{ch:introduction}

The advent of Large Language Models (LLMs) has fundamentally transformed the landscape of Natural Language Processing (NLP) and information retrieval. Models such as OpenAI's GPT-4 \cite{GPT4omini}, Google's Gemini \cite{Gemini25}, and DeepSeek \cite{DeepSeekV3} have demonstrated unprecedented capabilities in generating human-like text, summarizing complex documents, and answering open-domain questions. However, the widespread adoption of these systems in critical domains—such as journalism, legal research, and education—is severely hindered by their tendency to generate plausible but factually incorrect information, a phenomenon widely known as ``hallucination'' \cite{Ji2023Survey}.

As LLMs are increasingly integrated into search engines, the ability to automatically verify the factual veracity of their outputs has become a paramount research challenge. This thesis addresses this challenge by \textbf{adapting} an open-source decoupled two-phase framework for automated fact-checking and \textbf{benchmarking} the performance of state-of-the-art LLMs (GPT-4, Gemini, DeepSeek) within this architecture.

\section{Motivation}
    
    \subsection{The Problem of LLM Hallucinations}
    While LLMs are fluent and coherent, they lack an inherent understanding of truth. They operate as probabilistic systems, predicting the next token based on statistical patterns learned during training rather than querying a structured knowledge base of verified facts. Consequently, these models often conflate correct information with outdated data, common misconceptions, or complete fabrications \cite{Ji2023Survey}.
    
    The risk is not merely academic; in high-stakes applications, hallucinations can lead to the propagation of misinformation, legal liabilities, and the erosion of user trust. For instance, an LLM might generate a summary of a news article that accurately captures the tone but invents specific dates or statistics. This ``fluency-factuality gap''—where the text reads perfectly but is factually wrong—makes manual verification difficult and time-consuming for human users.

    \subsection{The Need for Automated Fact-Checking}
    Manual fact-checking is unscalable given the volume of content generated by LLMs. Traditional fact-checking pipelines often rely on human annotators or static knowledge graphs, which cannot keep pace with the dynamic nature of information on the web \cite{FEVER}.
    
    Recent approaches have proposed using LLMs themselves as evaluators (LLM-as-a-Judge) \cite{LLMJudge}. However, existing evaluation frameworks often treat fact-checking as a monolithic task, asking a model to ``verify this text'' in a single step. This approach obscures the root cause of errors: did the model fail to notice a verifiable claim (extraction error), or did it fail to verify the claim against evidence (verification error)? 
    
    To address this, there is a need for a granular, agent-based evaluation framework that decouples \textit{extraction} from \textit{verification}. By isolating these phases, researchers can identify whether a model is better suited for the high-recall task of identifying claims or the high-precision task of judging them against ground truth.

\section{Objective and Research Questions}
    
    \subsection{Thesis Objective}
    The primary objective of this thesis is to perform a comparative analysis of three leading Large Language Models—\textbf{GPT-4}, \textbf{Google Gemini}, and \textbf{DeepSeek}—to determine their efficacy in an automated fact-checking pipeline.

    \begin{table}[ht]
        \centering
        \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{2.5cm}|>{\raggedright\arraybackslash}p{3.5cm}|>{\raggedright\arraybackslash}X|}
            \hline
            \textbf{Provider} & \textbf{Model Name} & \textbf{Version / Endpoint} \\ 
            \hline
            OpenAI & GPT-4o mini & \texttt{gpt-4o-mini-2024-07-18} \\ 
            \hline
            Google & Gemini 2.5 Flash & \texttt{gemini-2.5-flash-001} \\ 
            \hline
            DeepSeek & DeepSeek-V3.2 & \texttt{deepseek-chat} (V3.2 API) \\ 
            \hline
        \end{tabularx}
        \caption{Specifications of the Large Language Models evaluated in this study.}
        \label{tab:models_specs}
    \end{table}
    
    Unlike previous studies that evaluate end-to-end performance, this research introduces and utilizes the \textbf{TruthLens Framework}, a novel agent-based architecture implementing a \textbf{Decoupled Two-Phase Evaluation Approach}. We leverage the \textit{BingCheck} dataset \cite{BingCheck} to assess each model's performance in:
    \begin{enumerate}
        \item \textbf{Phase 1 (Extraction):} Identifying verifiable factual claims from unstructured sentences \cite{Metropolitansky2025}.
        \item \textbf{Phase 2 (Verification):} Determining the accuracy of the LLMs in assessing the veracity of those claims against a standardized ground truth, utilizing a search-augmented verification approach \cite{SAFE2024}.
    \end{enumerate}
    This thesis aims to establish which model offers the optimal balance between precision (trustworthiness) and recall (coverage) for building reliable automated fact-checking agents.
    
    \subsection{Research Questions}
    To achieve the stated objective, this thesis answers the following three research questions (RQs):
    
    \begin{description}
        \item[RQ1: Claim Extraction] Which LLM most accurately identifies verifiable factual claims from text at the sentence level? \\
        \textit{Specifically, how do the models compare in terms of Recall (avoiding missed claims) versus Precision (avoiding subjective noise)?}
        
        \item[RQ2: Claim Verification] Which LLM most accurately assesses the veracity of identified claims when provided with claims?
        
        \item[RQ3: Qualitative Analysis] What are the common failure modes for each LLM in the fact-checking pipeline? \\
        \textit{Do certain models tend to be overly conservative (silence) or overly confident (hallucination)?}
    \end{description}
    
\section{Structure of the Thesis}
    The remainder of this thesis is structured as follows:
    
    \textbf{Chapter \ref{ch:background}} provides the theoretical background on Large Language Models, the phenomenon of hallucination, and the state-of-the-art in automated fact-checking, including a review of the SAFE framework \cite{SAFE2024} and the BingCheck dataset.
    
    \textbf{Chapter \ref{ch:methodology}} details the experimental methodology. It defines the \texttt{truthlens} framework, the decoupled evaluation strategy, and the specific definitions of metrics such as F1-Score and atomic claims used throughout the study.
    
    \textbf{Chapter \ref{ch:implementation}} describes the technical implementation of the evaluation pipeline, including the Python-based agent architecture, the integration of LLM APIs, and the automated benchmarking scripts.
    
    \textbf{Chapter \ref{ch:results}} presents the quantitative results of the experiments. It offers a statistical comparison of GPT-4, Gemini, and DeepSeek across both extraction and verification phases, highlighting the trade-offs between cost, speed, and accuracy across multiple experimental runs.
    
    \textbf{Chapter \ref{ch:discussion}} interprets the findings, discussing the implications of the ``precision-recall trade-off'' observed in the models. It also provides a qualitative error analysis of specific failure cases.
    
    \textbf{Chapter \ref{ch:conclusion}} summarizes the contributions of this work and suggests future directions for agent-based fact-checking systems.




\chapter{Background and State of the Art}
\label{ch:background}

    This chapter establishes the theoretical foundations of the thesis. It reviews the phenomenon of hallucination in Large Language Models (LLMs), analyzes the state-of-the-art methodologies for factual claim extraction and verification, and introduces the theoretical basis for the \texttt{truthlens} framework.


    \section{Large Language Models and Factuality}
    The capabilities of Large Language Models (LLMs) have scaled dramatically in recent years, driven by advancements in transformer architectures and the expansion of pre-training datasets. However, these models remain prone to ``hallucination''—the generation of text that is fluent and coherent but factually incorrect \cite{Ji2023Survey}.
    
    Early research in Natural Language Generation (NLG) primarily categorized these errors relative to a source document (e.g., in summarization tasks). This produced the standard distinction between \textbf{Intrinsic} and \textbf{Extrinsic} hallucinations (Table \ref{tab:hallucination_types}).
    
    \begin{table}[ht]
        \centering
        \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{3.5cm}|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}X|}
            \hline
            \textbf{Type} & \textbf{Definition} & \textbf{Example Scenario} \\ 
            \hline
            \textbf{Intrinsic Hallucination} & 
            The generated output directly contradicts the source material or input context. & 
            Input: ``The cat is black.'' \newline Output: ``The cat is white.'' \\ 
            \hline
            \textbf{Extrinsic Hallucination} & 
            The generated output cannot be verified from the source (neither supported nor contradicted), effectively constituting a fabrication. & 
            Input: ``The cat is black.'' \newline Output: ``The cat is named Felix.'' (Source never mentions a name). \\ 
            \hline
        \end{tabularx}
        \caption{Classic classification of Hallucinations as defined by \cite{Ji2023Survey}.}
        \label{tab:hallucination_types}
    \end{table}
    
    However, as LLMs have evolved into open-ended agents that rely on world knowledge rather than just a single input document, recent literature argues for a refined taxonomy. \cite{Huang2024Survey} distinguish between two dominant failure modes in modern Generative AI:
    
    \begin{enumerate}
        \item \textbf{Faithfulness Hallucination:} The model fails to follow user instructions or maintains internal inconsistency (e.g., self-contradiction).
        \item \textbf{Factuality Hallucination:} The model generates content that contradicts verifiable real-world facts. This manifests as \textit{Factual Fabrication} (inventing non-existent entities) or \textit{Factual Contradiction} (misstating established relations).
    \end{enumerate}
    
    This thesis focuses specifically on detecting and mitigating \textbf{Factuality Hallucinations}. Unlike faithfulness errors, which can often be solved via prompt engineering, factuality errors require external verification pipelines—such as the \texttt{truthlens} framework proposed in this study—to audit the model's output against reliable ground truth.

    
    \section{Phase 1: Factual Claim Extraction}
    The first step in any fact-checking pipeline is identifying \textit{what} needs to be checked. Raw LLM output often interweaves verifiable facts with opinions, reasoning, and conversational filler. Evaluating the text as a monolithic block often leads to imprecise results.
    
    \subsection{Literature Review: The Atomic Claim Approach}
    State-of-the-art research suggests that effective evaluation requires breaking down long-form text into "atomic claims." An atomic claim is defined as a specific, verifiable proposition that is fully self-contained \cite{Metropolitansky2025}.
    
    Recent work by Metropolitansky and Larson (2025) introduced \textbf{Claimify}, a framework that formalized this process. They identified that naive sentence splitting is insufficient because it strips necessary context (e.g., resolving pronouns like "he" or "it"). Their research proposed a multi-stage pipeline consisting of:
    \begin{enumerate}
        \item \textbf{Selection:} Filtering out subjective or non-verifiable sentences.
        \item \textbf{Disambiguation:} Resolving coreferences (e.g., changing "He released the album" to "Kendrick Lamar released the album") to make sentences self-contained.
        \item \textbf{Decomposition:} Breaking complex compound sentences into individual atomic facts.
    \end{enumerate}
    This thesis adopts the Claimify methodology for Phase 1, positing that higher-quality extraction leads to more accurate verification downstream.
    
    \subsection{The BingCheck Dataset}
    To evaluate extraction performance, a rigorous ground truth is required. This study leverages the \textbf{BingCheck} corpus \cite{BingCheck}, a high-quality dataset originally designed for evaluating Retrieval-Augmented Generation (RAG) systems across diverse domains such as finance, history, and science.
    
    However, the original BingCheck dataset focuses on document-level verification. For the specific task of atomic claim extraction (Phase 1), this thesis utilizes the sentence-level annotations derived from BingCheck and published by \cite{Metropolitansky2025} alongside the Claimify framework. This derived corpus provides expert-labeled binary classifications for sentences (contains claim / does not contain claim), serving as the standard ground truth for our precision and recall measurements.
    
    \section{Phase 2: Factual Claim Verification}
    Once atomic claims are extracted, they must be verified against external evidence. Relying on an LLM's internal parametric knowledge is insufficient due to the "knowledge cutoff" problem and the risk of reinforcing existing hallucinations.
    
    \subsection{Literature Review: Search-Augmented Verification}
    The prevailing standard for automated verification is \textbf{Search-Augmented Factuality Evaluation (SAFE)}, proposed by researchers at Google DeepMind \cite{SAFE2024}. SAFE introduces the paradigm of using an LLM agent to interact with a search engine (e.g., Google Search) to verify claims.
    
    The SAFE framework operates on a "reasoning loop":
    \begin{enumerate}
        \item \textbf{Query Generation:} The model formulates search queries based on the claim.
        \item \textbf{Evidence Retrieval:} Search results are parsed to find supporting or refuting evidence.
        \item \textbf{Reasoning:} The model compares the claim against the retrieved evidence to issue a verdict (Supported, Irrelevant, or Not Supported).
    \end{enumerate}
    Wei et al. (2024) demonstrated that LLM agents using this method can achieve human-level performance in rating long-form factuality, significantly outperforming human crowdsourced annotators in terms of cost and scalability. This thesis adapts the SAFE logic into the verification agent of the \texttt{truthlens} framework.

    \section{The \texttt{truthlens} Framework}
    To empirically evaluate the performance of GPT-4, Gemini, and DeepSeek, this thesis employs the \textbf{TruthLens} framework. TruthLens is an adaptation of the open-source \textit{ClaimeAI} repository, which implements a decoupled fact-checking pipeline.
    
    For this study, the original framework—which relied exclusively on GPT-4o-mini—was extended to support a multi-model architecture, allowing for the direct comparison of different Large Language Models as the underlying cognitive engine.

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{truthlens_architecture.png} 
        \caption{The \texttt{truthlens} Decoupled Framework Architecture. Phase 1 extracts atomic claims, which are passed to Phase 2 for search-augmented verification.}
        \label{fig:truthlens_architecture}
    \end{figure}
    
    \subsection{Agent 1: The Claim Extractor}
    The Claim Extractor agent implements the \textit{Claimify} methodology described in Section 2.2. It utilizes a graph-based workflow to process unstructured text through specific nodes for selection, disambiguation, and decomposition \cite{Metropolitansky2025}. By adhering to this rigorous extraction logic, the agent ensures that only atomic, context-independent claims are passed to the next phase.
    
    \subsection{Agent 2: The Claim Verifier}
    The Claim Verifier agent implements the search-augmented reasoning loop described in Section 2.3 \cite{SAFE2024}. Designed as an autonomous agent, it iteratively generates search queries, evaluates the sufficiency of retrieved evidence, and issues a final verdict (Supported, Refuted, or Insufficient Information) based on the specific capabilities of the LLM being tested.



\chapter{Methodology}
\label{ch:methodology}
    This chapter details the experimental framework designed to answer the research questions. It defines the specific Large Language Models (LLMs) selected for comparison, the structure of the TruthLens agentic framework, and the rigorous metric definitions used to evaluate performance in both extraction and verification phases.
    

    \section{Experimental Framework}
    To perform a fair and reproducible comparative analysis, this study utilizes \textbf{TruthLens}—an experimental adaptation of the open-source \textit{ClaimeAI} repository \cite{ClaimeAI_Repo}.\footnote{Original repository available at: \url{https://github.com/BharathxD/ClaimeAI}}
    
    While the original repository provided a baseline implementation for single-model execution (utilizing only GPT-4o-mini), this study introduces significant architectural extensions required for comparative analysis:
    \begin{itemize}
        \item \textbf{Multi-Provider Abstraction:} Refactoring the underlying LLM interface to support the execution of Google Gemini and DeepSeek models alongside OpenAI.
        \item \textbf{Decoupled Benchmarking Pipeline:} Developing scripts to isolate Phase 1 (Extraction) and Phase 2 (Verification) metrics for independent evaluation.
        \item \textbf{Dataset Adaptation:} Integrating the BingCheck ground-truth dataset to enable quantitative precision/recall measurement for the claim extraction phase.
    \end{itemize}
    
    \subsection{Experimental Design and Variables}
    The experiment is designed as a comparative study where the \textbf{Independent Variable} is the Large Language Model used as the cognitive engine for the agents. The \textbf{Dependent Variables} are the performance metrics (Precision, Recall, F1-Score, and Accuracy) achieved by each model.
    
    To ensure internal validity, the following controls were strictly enforced:
    \begin{itemize}
        \item \textbf{Identical Prompts:} All three models received the exact same system instructions and few-shot examples for both extraction and verification tasks. No model-specific prompt engineering was applied.
        \item \textbf{Deterministic Sampling:} To minimize variability in generation, the temperature settings were standardized (where applicable) to prioritize the deterministic outputs.
    \end{itemize}
    
    \subsection{LLM Providers}
    The study benchmarks three models representing different architectural philosophies (Proprietary vs. Open Weights) and cost profiles. The specific versions used for all experiments are listed in Table \ref{tab:models_specs} (see Chapter 1).
    
    \section{Dataset Creation and Sampling}
    \subsection{Source Dataset (BingCheck)}
    The source text for this experiment is derived from the \textbf{BingCheck} dataset \cite{BingCheck}. BingCheck was selected because it contains high-quality, human-annotated fact-checking instances spanning diverse domains (Science, History, Finance).
    
    \subsection{Sampling Strategy (Phase 1)}
    Due to the computational intensity of agentic workflows, a stratified random sample was utilized. Using a fixed random seed (\texttt{random\_state=42}) for reproducibility, $N=150$ sentences were sampled from the source corpus. 
    
    \subsection{Ground Truth Generation}
    A critical challenge in automated fact-checking is establishing a reliable "Gold Standard." This study adopts a hybrid approach:
    
    \begin{itemize}
        \item \textbf{Phase 1 (Extraction):} The ground truth for extraction was derived from the expert annotations published by the authors of the \textit{Claimify} framework \cite{Metropolitansky2025}. Each of the 150 sampled sentences possesses a binary label (\textit{True/False}) indicating whether it contains a factual claim.
        
        \item \textbf{Phase 2 (Verification):} For the verification phase, a subset of 100 atomic claims will be selected from the best-performing extractor. To ensure a fair evaluation of the model's safety behaviors, the ground truth will be established via manual human annotation using a \textbf{three-label schema}:
        \begin{itemize}
            \item \textbf{Supported:} The claim is explicitly confirmed by authoritative sources.
            \item \textbf{Refuted:} The claim is explicitly contradicted by authoritative sources.
            \item \textbf{Insufficient Information:} No reliable evidence could be found to either support or refute the claim (e.g., unverifiable private information or ambiguous predictions).
        \end{itemize}
        This alignment ensures that if a model correctly identifies a claim as unverifiable, it is penalized not as an error but rewarded as a correct assessment of uncertainty.
    \end{itemize}
    
    \section{Decoupled Two-Phase Evaluation}
    The \texttt{truthlens} framework executes the evaluation in two distinct, sequential phases.
    
    \subsection{Phase 1: Claim Extraction (The "Recall" Task)}
    The objective of Phase 1 is to convert unstructured input text into a list of \textit{atomic claims}. An atomic claim must be verifiable and decontextualized.
    For this phase, the \texttt{claim\_extractor} agent processes the 150 sampled sentences. The output is evaluated as a binary classification task: for every sentence in the ground truth, did the model correctly identify it as containing a claim?
    
    \subsection{Phase 2: Claim Verification (The "Precision" Task)}
    The objective of Phase 2 is to verify the accuracy of the claims. The \texttt{claim\_verifier} agent utilizes a search-augmented loop (Search $\rightarrow$ Read $\rightarrow$ Reason) to compare the claim against web evidence. 
    The output is a multi-class classification: \textit{Supported}, \textit{Refuted}, or \textit{Insufficient Information}. The model's verdict is compared against the manually annotated ground truth to calculate accuracy.


    \section{Evaluation Metrics}
    Reliable measurement requires distinct metrics for each phase.
    
    \subsection{Phase 1: Claim Extraction Metrics (Binary)}
    In Phase 1, the task is treated as a binary classification problem where the \textbf{Positive Class} represents the presence of a verifiable claim, and the \textbf{Negative Class} represents its absence (e.g., subjective opinions or chitchat). To fully capture the Precision-Recall trade-off (RQ1), metrics are calculated for both classes.
    
    \subsubsection{Positive Class Metrics (Target: Factual Claims)}
    These metrics evaluate the model's ability to correctly identify factual content:
    \begin{itemize}
        \item \textbf{Precision (+):} $\frac{TP}{TP + FP}$ \\
        \textit{Interpretation:} \textbf{Resistance to Noise.} Of the sentences the LLM classified as "factual," how many actually were? High precision implies the model avoids hallucinating claims in subjective text.
        
        \item \textbf{Recall (+):} $\frac{TP}{TP + FN}$ \\
        \textit{Interpretation:} \textbf{Coverage.} Of the sentences that actually contained facts, how many did the LLM find?
        
        \item \textbf{F1-Score (+):} The harmonic mean of Precision (+) and Recall (+). This is the primary ranking metric for extraction performance.
    \end{itemize}
    
    \subsubsection{Negative Class Metrics (Target: Non-Factual Content)}
    These metrics evaluate the model's ability to correctly reject non-factual content:
    \begin{itemize}
        \item \textbf{Precision (-):} $\frac{TN}{TN + FN}$ \\
        \textit{Interpretation:} \textbf{Negative Predictive Value.} When the model predicts a sentence is non-factual, how often is it correct?
        
        \item \textbf{Recall (-):} $\frac{TN}{TN + FP}$ \\
        \textit{Interpretation:} \textbf{Filtering Capability.} Of the subjective/non-factual sentences in the dataset, how many did the model correctly ignore? This directly measures the model's ability to filter out subjective noise.
    
        \item \textbf{F1-Score (-):} $2 \times \frac{Precision(-) \times Recall(-)}{Precision(-) + Recall(-)}$ \\
        \textit{Interpretation:} The harmonic mean for the negative class. A high score here indicates a balanced ability to filter noise without aggressively rejecting valid facts.
    \end{itemize}
    
    \subsection{Phase 2: Claim Verification Metrics (Multi-Class)}
    In Phase 2, the model must classify a claim into one of three categories: \textit{Supported}, \textit{Refuted}, or \textit{Insufficient Information}. 
    
    The primary metric is \textbf{Accuracy}, calculated as the ratio of correct verdicts to total claims:
    \begin{equation}
        Accuracy = \frac{\text{Correct Verdicts}}{\text{Total Claims Verified}}
    \end{equation}
    
    Additionally, we calculate the \textbf{Macro-Averaged F1-Score} across all three classes. This ensures that the model is evaluated not just on its ability to verify facts, but also on its ability to correctly identify when evidence is missing (Recall for the ``Insufficient'' class), preventing bias towards the majority class.
    
    \section{Reliability and Reproducibility}
    Given that Large Language Models are non-deterministic by nature, a single experimental run may not accurately reflect the model's stable performance characteristics. To mitigate this variability and ensure statistical reliability, the entire evaluation pipeline (Phase 1 and Phase 2) is executed in \textbf{three independent runs} ($k=3$).
    
    For each metric $M$ (Precision, Recall, F1-Score), the reported result is the mean across all three runs:
    \begin{equation}
        M_{final} = \frac{1}{k} \sum_{i=1}^{k} M_i
    \end{equation}
    This multi-run approach allows for the calculation of standard deviation to quantify the stability of each model's extraction and verification capabilities.
    


\chapter{Implementation}
\label{ch:implementation}
    \section{System Architecture}
    \section{LLM Abstraction Layer (Adapting \texttt{llm.py})}
    \section{Benchmark Automation}
        \subsection{The Benchmark Runner Script}
        \subsection{Data Storage in Master CSV}
    \section{Analysis and Visualization Script}

\chapter{Results and Evaluation}
\label{ch:results}
    \section{Phase 1: Claim Extraction Performance}
        \subsection{Quantitative Metric Comparison (Table)}
        \subsection{Visual Comparison (Bar Charts)}
        \subsection{Reliability Analysis Across Multiple Runs}
    \section{Phase 2: Claim Verification Performance}
        \subsection{Quantitative Metric Comparison (Table)}
        \subsection{Visual Comparison (Bar Charts)}
    \section{Experimental Cost and Time Analysis}

\chapter{Discussion}
\label{ch:discussion}
    \section{Comparative Analysis of LLMs}
        \subsection{Best Performing LLM for Extraction}
        \subsection{Best Performing LLM for Verification}
    \section{Qualitative Error Analysis}
        \subsection{Common Failures in Claim Extraction}
        \subsection{Common Failures in Claim Verification (Analysis of \texttt{reason} field)}
    \section{Limitations of the Study}
        \subsection{Impact of Sample Size (N=150)}
        \subsection{Constraints of the \texttt{truthlens} Agents}

\chapter{Conclusion and Future Work}
\label{ch:conclusion}
    \section{Summary of Findings}
        \subsection{Answering Research Question 1 (Extraction)}
        \subsection{Answering Research Question 2 (Verification)}
        \subsection{Answering Research Question 3 (Qualitative)}
    \section{Future Work}
        \subsection{Scaling the Experiment}
        \subsection{Evaluating Other Agent Architectures}

% --- Appendices ---
\appendix
\chapter{Appendix}
    \section{Full Benchmark Results Table}
    \section{Benchmark Runner Script (\texttt{run\_my\_thesis.py})}
    \section{Analysis Script (\texttt{analyze\_results.py})}

% --- Bibliography ---
\clearpage
This is where your .bib file would be included
\bibliographystyle{apalike}
\bibliography{references}

\end{document}